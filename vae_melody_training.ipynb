{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true,
   "mount_file_id": "1ufRp8OqWN5n9VRDYzxSC6uxmJv6KigAS",
   "authorship_tag": "ABX9TyMFm6rzqHuaKV3FvU+9nOfz",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mdsenelen/Melody-Generation-VAE-for-Musician-s-Assistant/blob/main/vae_melody_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ],
   "metadata": {
    "id": "sAV1JPw8-0Lg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcCtQa4HFBON",
    "outputId": "09c7b0f9-2a1d-4ad0-87e7-50c87a330ca2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !nvcc --version\n"
   ],
   "metadata": {
    "id": "HLFHmQGY-6hI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install torch==2.0.0+cu118 torchvision --extra-index-url https://download.pytorch.org/whl/cu118\n"
   ],
   "metadata": {
    "id": "Z3fH05g7_Bfc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip uninstall nvidia-cudnn-cu12\n"
   ],
   "metadata": {
    "id": "iT4vP9ue_NJQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip uninstall -y torch torchvision torchaudio\n",
    "# !pip install torch==2.0.1+cu118 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n"
   ],
   "metadata": {
    "id": "0EayAeyv_QY5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Musician's Assistant VAE Trainer with Thesis Visualizations\n",
    "\n",
    "Combined notebook for:\n",
    "1. Web-optimized VAE training with consistent 128x256 spectrogram outputs\n",
    "2. Comprehensive visualizations for academic research\n",
    "\"\"\"\n",
    "\n",
    "# 1. IMPORT REQUIRED LIBRARIES\n",
    "!pip install torchaudio librosa numpy matplotlib plotly scikit-learn soundfile tensorboard seaborn umap-learn fpdf\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, Audio, display\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "from tqdm import tqdm, trange\n",
    "import zipfile\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "from fpdf import FPDF\n",
    "import glob\n",
    "\n",
    "# 2. CONFIGURATION (Web-App Compatible)\n",
    "CONFIG = {\n",
    "    \"audio\": {\n",
    "        \"sample_rate\": 22050,\n",
    "        \"n_fft\": 2048,\n",
    "        \"hop_length\": 512,\n",
    "        \"win_length\": 1024,\n",
    "        \"n_mels\": 128,\n",
    "        \"fmin\": 30,\n",
    "        \"fmax\": 8000,\n",
    "        \"max_frames\": 256\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"latent_dim\": 256,\n",
    "        \"input_shape\": [1, 128, 256],\n",
    "        \"batch_size\": 32,\n",
    "        \"init_lr\": 3e-4,\n",
    "        \"num_epochs\": 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. FIXED MODEL ARCHITECTURE\n",
    "class WebVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder with fixed output dimensions\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        # Fixed latent space\n",
    "        self.fc_mu = nn.Linear(256*16*32, CONFIG['model']['latent_dim'])\n",
    "        self.fc_logvar = nn.Linear(256*16*32, CONFIG['model']['latent_dim'])\n",
    "        self.latent_dim = CONFIG['model']['latent_dim']\n",
    "\n",
    "        # Decoder with fixed output (128x256)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=(5,4), stride=(1,2), padding=(2,1)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_mu(x), self.fc_logvar(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = z.view(-1, 256, 16, 32)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# 4. DATA LOADING AND PROCESSING\n",
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, mel_array):\n",
    "        self.mels = mel_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = self.mels[idx]\n",
    "        return torch.tensor(mel, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    print(\"\ud83d\udcc1 Loading and processing audio files...\")\n",
    "    mel_list = []\n",
    "\n",
    "    wav_files = [f for f in os.listdir(data_path) if f.lower().endswith('.wav')]\n",
    "\n",
    "    for file in tqdm(wav_files, desc=\"Processing Audio Files\"):\n",
    "        try:\n",
    "            filepath = os.path.join(data_path, file)\n",
    "            y, _ = librosa.load(filepath, sr=CONFIG['audio']['sample_rate'])\n",
    "            y = librosa.util.normalize(y) * 0.707  # -3dB normalizasyon\n",
    "\n",
    "            mel = librosa.feature.melspectrogram(\n",
    "                y=y, sr=CONFIG['audio']['sample_rate'],\n",
    "                n_mels=CONFIG['audio']['n_mels'],\n",
    "                n_fft=CONFIG['audio']['n_fft'],\n",
    "                hop_length=CONFIG['audio']['hop_length'],\n",
    "                win_length=CONFIG['audio']['win_length'],\n",
    "                fmin=CONFIG['audio']['fmin'],\n",
    "                fmax=CONFIG['audio']['fmax'],\n",
    "                power=1.0)\n",
    "\n",
    "            mel_db = librosa.amplitude_to_db(mel, ref=np.max, top_db=80)\n",
    "            mel_db = np.clip(mel_db, -40, 0)\n",
    "            mel_norm = (mel_db + 40) / 40\n",
    "\n",
    "            if mel_norm.shape[1] < CONFIG['audio']['max_frames']:\n",
    "                pad_width = CONFIG['audio']['max_frames'] - mel_norm.shape[1]\n",
    "                mel_norm = np.pad(mel_norm, ((0, 0), (0, pad_width)),\n",
    "                                 mode='constant', constant_values=-40/40+1)\n",
    "            else:\n",
    "                mel_norm = mel_norm[:, :CONFIG['audio']['max_frames']]\n",
    "\n",
    "            mel_list.append(mel_norm)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n\u26a0\ufe0f Error processing {file}: {str(e)}\")\n",
    "\n",
    "    return np.array(mel_list)\n",
    "\n",
    "# 5. VISUALIZATION TOOLS\n",
    "class Visualizer:\n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(os.path.join(log_dir, \"visualizations\"), exist_ok=True)\n",
    "\n",
    "    def plot_training_curves(self, train_losses, val_losses, metrics):\n",
    "        plt.figure(figsize=(18, 12))\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(train_losses, label='Train Loss', alpha=0.8)\n",
    "        plt.plot(val_losses, label='Val Loss', alpha=0.8)\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(metrics['psnr'], label='PSNR', color='green')\n",
    "        plt.title('PSNR Over Time')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('PSNR (dB)')\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(metrics['ssim'], label='SSIM', color='purple')\n",
    "        plt.title('SSIM Over Time')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('SSIM')\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        ax = plt.gca()\n",
    "        ax.plot(metrics['psnr'], color='green', label='PSNR')\n",
    "        ax.set_ylabel('PSNR (dB)', color='green')\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(metrics['ssim'], color='purple', label='SSIM')\n",
    "        ax2.set_ylabel('SSIM', color='purple')\n",
    "        plt.title('Quality Metrics Correlation')\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.log_dir, \"visualizations\", \"training_metrics.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    def plot_latent_space(self, latent_vectors):\n",
    "        pca = PCA(n_components=3).fit_transform(latent_vectors)\n",
    "        tsne = TSNE(n_components=3).fit_transform(latent_vectors)\n",
    "        umap_emb = umap.UMAP(n_components=3).fit_transform(latent_vectors)\n",
    "\n",
    "        fig = make_subplots(rows=1, cols=3,\n",
    "                           specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
    "                           subplot_titles=('PCA', 't-SNE', 'UMAP'))\n",
    "\n",
    "        for i, (emb, name) in enumerate(zip([pca, tsne, umap_emb], ['PCA', 't-SNE', 'UMAP'])):\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=emb[:, 0], y=emb[:, 1], z=emb[:, 2],\n",
    "                    mode='markers',\n",
    "                    marker=dict(size=4, opacity=0.8),\n",
    "                    name=name\n",
    "                ),\n",
    "                row=1, col=i+1\n",
    "            )\n",
    "\n",
    "        fig.update_layout(height=600, width=1800, title_text=\"3D Latent Space Projections\")\n",
    "        fig.write_html(os.path.join(self.log_dir, \"visualizations\", \"3d_latent.html\"))\n",
    "\n",
    "        plt.figure(figsize=(18, 5))\n",
    "        for i, (emb, name) in enumerate(zip([pca[:,:2], tsne[:,:2], umap_emb[:,:2]], ['PCA', 't-SNE', 'UMAP'])):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            sns.kdeplot(x=emb[:, 0], y=emb[:, 1], cmap=\"viridis\", fill=True, thresh=0.1)\n",
    "            plt.title(f'{name} Projection')\n",
    "        plt.savefig(os.path.join(self.log_dir, \"visualizations\", \"2d_latent.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    def plot_spectrogram_comparison(self, original, reconstructed, epoch=None):\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "        plt.subplot(3, 1, 1)\n",
    "        librosa.display.specshow(original, sr=CONFIG['audio']['sample_rate'],\n",
    "                               hop_length=CONFIG['audio']['hop_length'],\n",
    "                               x_axis='time', y_axis='mel', cmap='magma')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Original Spectrogram')\n",
    "\n",
    "        plt.subplot(3, 1, 2)\n",
    "        librosa.display.specshow(reconstructed, sr=CONFIG['audio']['sample_rate'],\n",
    "                               hop_length=CONFIG['audio']['hop_length'],\n",
    "                               x_axis='time', y_axis='mel', cmap='magma')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Reconstructed Spectrogram')\n",
    "\n",
    "        plt.subplot(3, 1, 3)\n",
    "        diff = np.abs(original - reconstructed)\n",
    "        librosa.display.specshow(diff, sr=CONFIG['audio']['sample_rate'],\n",
    "                               hop_length=CONFIG['audio']['hop_length'],\n",
    "                               x_axis='time', y_axis='mel', cmap='coolwarm')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Absolute Difference')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fname = f\"spectrogram_comparison{'_epoch'+str(epoch) if epoch else ''}.png\"\n",
    "        plt.savefig(os.path.join(self.log_dir, \"visualizations\", fname))\n",
    "        plt.close()\n",
    "\n",
    "    def create_spectrogram_animation(self, spectrograms, filename=\"spectrogram_evolution.gif\"):\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        ax = plt.gca()\n",
    "\n",
    "        def update(frame):\n",
    "            ax.clear()\n",
    "            librosa.display.specshow(spectrograms[frame],\n",
    "                                   sr=CONFIG['audio']['sample_rate'],\n",
    "                                   hop_length=CONFIG['audio']['hop_length'],\n",
    "                                   x_axis='time', y_axis='mel', cmap='magma')\n",
    "            plt.title(f'Epoch {frame+1}')\n",
    "\n",
    "        anim = FuncAnimation(fig, update, frames=len(spectrograms), interval=500)\n",
    "        anim.save(os.path.join(self.log_dir, \"visualizations\", filename), writer='pillow')\n",
    "        return HTML(anim.to_jshtml())\n",
    "\n",
    "# 6. TRAINING AND EVALUATION\n",
    "def compute_metrics(target, recon):\n",
    "    target_np = target.squeeze().cpu().numpy()\n",
    "    recon_np = recon.squeeze().cpu().numpy()\n",
    "    psnr_score = psnr(target_np, recon_np, data_range=1.0)\n",
    "    ssim_score = ssim(target_np, recon_np, data_range=1.0, channel_axis=0)\n",
    "    return psnr_score, ssim_score\n",
    "\n",
    "def spectral_convergence_loss(input, target):\n",
    "    return torch.norm(target - input, p='fro') / (torch.norm(target, p='fro') + 1e-6)\n",
    "\n",
    "def spectral_magnitude_loss(input, target):\n",
    "    return F.l1_loss(torch.log1p(input), torch.log1p(target))\n",
    "\n",
    "def compute_loss(recon, target, mu, logvar):\n",
    "    mse_loss = F.mse_loss(recon, target)\n",
    "    sc_loss = spectral_convergence_loss(recon, target)\n",
    "    sm_loss = spectral_magnitude_loss(recon, target)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return {\n",
    "        'loss': mse_loss + 0.5*sc_loss + 0.5*sm_loss + 0.001*kl_loss,\n",
    "        'mse': mse_loss,\n",
    "        'sc': sc_loss,\n",
    "        'sm': sm_loss,\n",
    "        'kl': kl_loss\n",
    "    }\n",
    "\n",
    "# 7. MAIN TRAINING FUNCTION\n",
    "def train_for_web_and_thesis(data_path, output_dir):\n",
    "    # Initialize\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load data\n",
    "    mels = load_dataset(data_path)\n",
    "    split_idx = int(0.8 * len(mels))\n",
    "    train_dataset = MelSpectrogramDataset(mels[:split_idx])\n",
    "    val_dataset = MelSpectrogramDataset(mels[split_idx:])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=CONFIG['model']['batch_size'],\n",
    "                            shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=CONFIG['model']['batch_size'])\n",
    "\n",
    "    # Model and optimizer\n",
    "    model = WebVAE().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['model']['init_lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    # Setup logging\n",
    "    log_dir = os.path.join(output_dir, datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    visualizer = Visualizer(log_dir)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_psnrs = []\n",
    "    val_ssims = []\n",
    "    best_samples = []\n",
    "\n",
    "    for epoch in trange(CONFIG['model']['num_epochs'], desc=\"Training\"):\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            if batch.shape[-2:] != (128, 256):\n",
    "                batch = F.interpolate(batch, size=(128,256))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(batch)\n",
    "            loss_dict = compute_loss(recon, batch, mu, logvar)\n",
    "            loss_dict['loss'].backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(loss_dict['loss'].item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        epoch_val_recons = []\n",
    "        epoch_val_targets = []\n",
    "        latent_vectors = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                if batch.shape[-2:] != (128, 256):\n",
    "                    batch = F.interpolate(batch, size=(128,256))\n",
    "\n",
    "                recon, mu, logvar = model(batch)\n",
    "                loss_dict = compute_loss(recon, batch, mu, logvar)\n",
    "                epoch_val_losses.append(loss_dict['loss'].item())\n",
    "                epoch_val_recons.append(recon.cpu())\n",
    "                epoch_val_targets.append(batch.cpu())\n",
    "                latent_vectors.append(mu.cpu().numpy())\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        avg_val_loss = np.mean(epoch_val_losses)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if epoch_val_recons:\n",
    "            recon_all = torch.cat(epoch_val_recons)\n",
    "            target_all = torch.cat(epoch_val_targets)\n",
    "            psnr_score, ssim_score = compute_metrics(target_all, recon_all)\n",
    "            val_psnrs.append(psnr_score)\n",
    "            val_ssims.append(ssim_score)\n",
    "\n",
    "            # Store best sample\n",
    "            best_idx = np.argmin([F.mse_loss(r, t) for r, t in zip(epoch_val_recons, epoch_val_targets)])\n",
    "            best_samples.append(epoch_val_recons[best_idx][0].squeeze().numpy())\n",
    "\n",
    "        # Visualizations\n",
    "        if epoch % 5 == 0 and epoch_val_recons:\n",
    "            sample_idx = 0\n",
    "            original = epoch_val_targets[0][sample_idx].squeeze().numpy()\n",
    "            recon = epoch_val_recons[0][sample_idx].squeeze().numpy()\n",
    "            visualizer.plot_spectrogram_comparison(original, recon, epoch)\n",
    "\n",
    "            if latent_vectors:\n",
    "                visualizer.plot_latent_space(np.concatenate(latent_vectors))\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Final visualizations\n",
    "    visualizer.create_spectrogram_animation(best_samples)\n",
    "    visualizer.plot_training_curves(\n",
    "        train_losses,\n",
    "        val_losses,\n",
    "        {'psnr': val_psnrs, 'ssim': val_ssims}\n",
    "    )\n",
    "\n",
    "    # Save final model and artifacts\n",
    "    torch.save({\n",
    "        'state_dict': model.state_dict(),\n",
    "        'config': CONFIG,\n",
    "        'class_name': 'WebVAE'\n",
    "    }, os.path.join(log_dir, \"web_model.pt\"))\n",
    "\n",
    "    with open(os.path.join(log_dir, \"web_audio_params.json\"), \"w\") as f:\n",
    "        json.dump(CONFIG['audio'], f)\n",
    "\n",
    "    # Generate PDF report\n",
    "    generate_report(log_dir)\n",
    "\n",
    "    print(f\"\u2705 Training completed! Results saved to: {log_dir}\")\n",
    "\n",
    "def generate_report(log_dir):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.cell(0, 10, \"VAE Melody Generation - Thesis Results\", ln=1, align='C')\n",
    "\n",
    "    vis_dir = os.path.join(log_dir, \"visualizations\")\n",
    "    images = glob.glob(os.path.join(vis_dir, \"*.png\")) + glob.glob(os.path.join(vis_dir, \"*.jpg\"))\n",
    "\n",
    "    for img in sorted(images):\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "        pdf.cell(0, 10, os.path.basename(img).replace('_', ' ').replace('.png', ''), ln=1)\n",
    "        pdf.image(img, x=10, y=30, w=180)\n",
    "\n",
    "    pdf.output(os.path.join(log_dir, \"thesis_results_report.pdf\"))\n",
    "\n",
    "# 8. WEB-COMPATIBLE AUDIO RECONSTRUCTION\n",
    "def web_reconstruct(mel):\n",
    "    \"\"\"Optimized for your web app backend\"\"\"\n",
    "    mel = (mel * 40) - 40  # Denormalize\n",
    "    mel = librosa.db_to_amplitude(mel)\n",
    "    audio = librosa.griffinlim(\n",
    "        librosa.feature.inverse.mel_to_stft(mel),\n",
    "        n_iter=32,  # Faster for web\n",
    "        hop_length=CONFIG['audio']['hop_length'],\n",
    "        win_length=CONFIG['audio']['win_length']\n",
    "    )\n",
    "    return audio\n",
    "\n",
    "# 9. RUN TRAINING\n",
    "if __name__ == \"__main__\":\n",
    "    # Mount Google Drive if needed\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Set your paths\n",
    "    DATA_DIR = \"/content/drive/MyDrive/vae_project/recordings\"\n",
    "    OUTPUT_DIR = \"/content/drive/MyDrive/vae_project/logs\"\n",
    "\n",
    "    # Start training\n",
    "    train_for_web_and_thesis(DATA_DIR, OUTPUT_DIR)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "YjrmjsTidXO4",
    "outputId": "4657f192-73a2-4666-e70c-cb19d4e3de2f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DR5MwcxaJ2N3",
    "outputId": "ba3201e7-6cba-46a2-8e66-02f677b517b0"
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"vae_melody_training.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1ufRp8OqWN5n9VRDYzxSC6uxmJv6KigAS\n",
    "\"\"\"\n",
    "\n",
    "# 1. REQUIRED LIBRARIES\n",
    "!pip install torchaudio librosa numpy matplotlib plotly scikit-learn soundfile\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import Audio, display\n",
    "import plotly.express as px\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display # Added import for librosa.display\n",
    "import unittest\n",
    "from tqdm import tqdm, trange\n",
    "from google.colab import drive\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchaudio  # Added missing import\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "USE_DRIVE = True\n",
    "DATA_DIR = \"/content/drive/MyDrive/vae_project/recordings\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/vae_project/logs\"\n",
    "\n",
    "# Mount Google Drive\n",
    "if USE_DRIVE:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# CALLBACK BASE\n",
    "class Callback:\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        pass\n",
    "\n",
    "class EarlyStopping(Callback):\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.stopped_epoch = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        val_loss = logs['val_loss']\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                logs['stop_training'] = True\n",
    "\n",
    "class ModelCheckpoint(Callback):\n",
    "    def __init__(self, model, filepath):\n",
    "        self.model = model\n",
    "        self.filepath = filepath\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        val_loss = logs['val_loss']\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(self.model.state_dict(), self.filepath)\n",
    "            print(f\"\ud83d\udccc Best model saved (val_loss={val_loss:.4f})\")\n",
    "\n",
    "class CSVLogger(Callback):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        with open(self.filename, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['epoch', 'train_loss', 'val_loss'])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        with open(self.filename, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch + 1, logs['train_loss'], logs['val_loss']])\n",
    "\n",
    "def load_dataset(data_path, sr=22050, n_mels=64, max_len=256):\n",
    "    \"\"\"\n",
    "    Loads audio files and converts them to normalized, fixed-size mel spectrograms.\n",
    "    \"\"\"\n",
    "    print(\"\ud83d\udcc1 Loading data...\")\n",
    "    mel_list = []\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
    "\n",
    "    wav_files = [f for f in os.listdir(data_path) if f.lower().endswith('.wav')]\n",
    "    if not wav_files:\n",
    "        raise ValueError(\"No .wav files found in the specified directory\")\n",
    "\n",
    "    for file in tqdm(wav_files, desc=\"Processing Audio Files\"):\n",
    "        try:\n",
    "            filepath = os.path.join(data_path, file)\n",
    "            y, _ = librosa.load(filepath, sr=sr)\n",
    "            mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "            mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "            # Normalize to [0, 1]\n",
    "            mel_db = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min())\n",
    "\n",
    "            # Pad or truncate to fixed time steps\n",
    "            if mel_db.shape[1] < max_len:\n",
    "                pad_width = max_len - mel_db.shape[1]\n",
    "                mel_db = np.pad(mel_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            else:\n",
    "                mel_db = mel_db[:, :max_len]\n",
    "\n",
    "            mel_list.append(mel_db)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n\u26a0\ufe0f Error processing {file}: {str(e)}\")\n",
    "\n",
    "    if not mel_list:\n",
    "        raise ValueError(\"No audio files could be processed\")\n",
    "\n",
    "    return np.array(mel_list)  # shape: (N, 64, max_len)\n",
    "\n",
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, mel_array):\n",
    "        self.mels = mel_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = self.mels[idx]\n",
    "        mel_tensor = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)  # shape: (1, 64, time)\n",
    "        return mel_tensor\n",
    "\n",
    "class EnhancedVAE(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 64, 256), latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # \u2192 (32, H/2, W/2)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # \u2192 (64, H/4, W/4)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # \u2192 (128, H/8, W/8)\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Compute conv output shape dynamically\n",
    "        self._conv_output_shape = self._get_conv_output_shape()\n",
    "        conv_output_dim = int(np.prod(self._conv_output_shape))\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(conv_output_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(conv_output_dim, latent_dim)\n",
    "\n",
    "        # Decoder input and reshape\n",
    "        self.decoder_input = nn.Linear(latent_dim, conv_output_dim)\n",
    "\n",
    "        # Decoder (mirror of encoder)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # \u00d72\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # \u00d72\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),    # \u00d72\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _get_conv_output_shape(self):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *self.input_shape)\n",
    "            output = self.encoder(dummy_input)\n",
    "            return output.shape[1:]  # (channels, H', W')\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, *self._conv_output_shape)\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "def main():\n",
    "    # 4. MODEL AND DATASET PREPARATION\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Data loading\n",
    "    try:\n",
    "        mels = load_dataset(DATA_DIR, max_len=256)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Split and wrap with Dataset\n",
    "    split_idx = int(0.8 * len(mels))\n",
    "    train_dataset = MelSpectrogramDataset(mels[:split_idx])\n",
    "    val_dataset = MelSpectrogramDataset(mels[split_idx:])\n",
    "\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Dynamically infer input shape from one sample\n",
    "    example_input = next(iter(train_loader))\n",
    "    input_shape = tuple(example_input.shape[1:])  # (1, 64, 256)\n",
    "\n",
    "    # Model\n",
    "    model = EnhancedVAE(input_shape=input_shape).to(device)\n",
    "    print(f\"Model initialized with input shape: {input_shape}\")\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    # 5. TRAINING LOOP\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_psnrs = []\n",
    "    val_ssims = []\n",
    "    latent_vectors = []\n",
    "\n",
    "    # Initialize callbacks\n",
    "    log_dir = os.path.join(OUTPUT_DIR, datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    model_filepath = os.path.join(log_dir, \"best_model.pt\")\n",
    "    csv_filepath = os.path.join(log_dir, \"training_log.csv\")\n",
    "    latent_path = os.path.join(log_dir, \"latent_vectors.npy\")\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=3),\n",
    "        ModelCheckpoint(model, model_filepath),\n",
    "        CSVLogger(csv_filepath)\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    def smooth_curve(points, factor=0.8):\n",
    "        smoothed_points = []\n",
    "        for point in points:\n",
    "            if smoothed_points:\n",
    "                previous = smoothed_points[-1]\n",
    "                smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "            else:\n",
    "                smoothed_points.append(point)\n",
    "        return smoothed_points\n",
    "\n",
    "    def compute_metrics(target, recon):\n",
    "        target_np = target.squeeze().cpu().numpy()\n",
    "        recon_np = recon.squeeze().cpu().numpy()\n",
    "\n",
    "        psnr_score = 0\n",
    "        ssim_score = 0\n",
    "        count = 0\n",
    "        for i in range(target_np.shape[0]):\n",
    "            img1 = target_np[i]\n",
    "            img2 = recon_np[i]\n",
    "            max_val = 1.0\n",
    "            psnr_score += psnr(img1, img2, data_range=max_val)\n",
    "            ssim_score += ssim(img1, img2, data_range=max_val)\n",
    "            count += 1\n",
    "\n",
    "        return psnr_score / count, ssim_score / count\n",
    "\n",
    "    # Training loop\n",
    "    if len(train_dataset) > 0:\n",
    "        for epoch in trange(num_epochs, desc=\"Training Epochs\"):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            num_batches = 0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                recon, mu, logvar = model(batch)\n",
    "                loss = F.mse_loss(recon, batch) + 0.5 * torch.sum(logvar.exp() + mu.pow(2) - 1 - logvar)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "            avg_train_loss = train_loss / max(1, num_batches)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss_accum = 0\n",
    "            val_batches = 0\n",
    "            val_recons = []\n",
    "            val_targets = []\n",
    "            val_mus = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    val_batch = val_batch.to(device)\n",
    "                    recon, mu, logvar = model(val_batch)\n",
    "                    loss = F.mse_loss(recon, val_batch) + 0.5 * torch.sum(logvar.exp() + mu.pow(2) - 1 - logvar)\n",
    "                    val_loss_accum += loss.item()\n",
    "                    val_batches += 1\n",
    "                    val_recons.append(recon.cpu())\n",
    "                    val_targets.append(val_batch.cpu())\n",
    "                    val_mus.append(mu.cpu())\n",
    "\n",
    "            avg_val_loss = val_loss_accum / max(1, val_batches)\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            # PSNR / SSIM\n",
    "            if val_targets:\n",
    "                recon_all = torch.cat(val_recons, dim=0)\n",
    "                target_all = torch.cat(val_targets, dim=0)\n",
    "                psnr_score, ssim_score = compute_metrics(target_all, recon_all)\n",
    "                val_psnrs.append(psnr_score)\n",
    "                val_ssims.append(ssim_score)\n",
    "                current_epoch_latents = torch.cat(val_mus, dim=0).numpy()\n",
    "                latent_vectors.append(current_epoch_latents)\n",
    "\n",
    "            logs = {\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'stop_training': False\n",
    "            }\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "                  f\"PSNR: {val_psnrs[-1] if val_psnrs else 0:.2f}, SSIM: {val_ssims[-1] if val_ssims else 0:.3f}\")\n",
    "\n",
    "            for cb in callbacks:\n",
    "                cb.on_epoch_end(epoch, logs)\n",
    "\n",
    "            if logs['stop_training']:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "            # Plotting with smoothing\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(smooth_curve(train_losses), label='Smoothed Train Loss', marker='o')\n",
    "            plt.plot(smooth_curve(val_losses), label='Smoothed Val Loss', marker='x')\n",
    "            plt.title('Training and Validation Loss (Smoothed)')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(log_dir, \"loss_plot.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # After training loop\n",
    "        if latent_vectors:\n",
    "            np.save(latent_path, latent_vectors[-1])\n",
    "            print(f\"Latent vectors saved to: {latent_path}\")\n",
    "\n",
    "            # Visualization\n",
    "            latents = latent_vectors[-1]\n",
    "            if latents.shape[0] >= 2:\n",
    "                pca = PCA(n_components=2).fit_transform(latents)\n",
    "                tsne = TSNE(n_components=2, perplexity=min(30, latents.shape[0]-1)).fit_transform(latents)\n",
    "\n",
    "                fig_pca = px.scatter(x=pca[:, 0], y=pca[:, 1], title=\"PCA of Latent Space\")\n",
    "                fig_tsne = px.scatter(x=tsne[:, 0], y=tsne[:, 1], title=\"t-SNE of Latent Space\")\n",
    "                fig_pca.write_html(os.path.join(log_dir, \"pca_latent.html\"))\n",
    "                fig_tsne.write_html(os.path.join(log_dir, \"tsne_latent.html\"))\n",
    "                fig_pca.show()\n",
    "                fig_tsne.show()\n",
    "\n",
    "        # Spectrogram Reconstruction Visuals\n",
    "        if val_recons and val_targets:\n",
    "            n = min(3, val_recons[-1].shape[0])\n",
    "            fig, axes = plt.subplots(nrows=n, ncols=2, figsize=(8, n*3))\n",
    "            samples = val_targets[-1][:n].cpu().squeeze(dim=1).numpy() # Squeeze channel dimension here\n",
    "            recons = val_recons[-1][:n].cpu().squeeze(dim=1).numpy() # Squeeze channel dimension here\n",
    "\n",
    "            print(f\"Shape of samples before plotting: {samples.shape}\") # Debug print\n",
    "            print(f\"Shape of recons before plotting: {recons.shape}\")   # Debug print\n",
    "\n",
    "            for i in range(n):\n",
    "                # Ensure data is 2D (64, 256) for specshow\n",
    "                sample_to_plot = samples[i]\n",
    "                recon_to_plot = recons[i]\n",
    "\n",
    "                if n > 1:\n",
    "                    librosa.display.specshow(sample_to_plot, ax=axes[i, 0])\n",
    "                    axes[i, 0].set_title(f\"Original {i}\")\n",
    "                    librosa.display.specshow(recon_to_plot, ax=axes[i, 1])\n",
    "                    axes[i, 1].set_title(f\"Reconstruction {i}\")\n",
    "                else:\n",
    "                    # Handle the case when n=1, axes is a 1D array of Axes objects\n",
    "                    librosa.display.specshow(sample_to_plot, ax=axes[0])\n",
    "                    axes[0].set_title(f\"Original {i}\")\n",
    "                    librosa.display.specshow(recon_to_plot, ax=axes[1])\n",
    "                    axes[1].set_title(f\"Reconstruction {i}\")\n",
    "\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(log_dir, \"spectrogram_reconstructions.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # Audio Reconstruction\n",
    "        if val_recons:\n",
    "            audio_files = []\n",
    "            n = min(3, val_recons[-1].shape[0])\n",
    "            recons_last_batch = val_recons[-1][:n].cpu().squeeze(dim=1).numpy()\n",
    "\n",
    "            for i in range(n):\n",
    "                mel = recons_last_batch[i]\n",
    "                if mel.ndim == 1:\n",
    "                    mel = mel.reshape(-1, 1)\n",
    "                mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "                audio = librosa.feature.inverse.mel_to_audio(\n",
    "                    librosa.db_to_power(mel_db), sr=22050, n_iter=32)\n",
    "                out_path = os.path.join(log_dir, f\"reconstructed_audio_{i}.wav\")\n",
    "                sf.write(out_path, audio, 22050)\n",
    "                audio_files.append(out_path)\n",
    "                print(f\"Reconstructed audio saved: {out_path}\")\n",
    "                display(Audio(out_path))\n",
    "\n",
    "            if audio_files:\n",
    "                zip_path = os.path.join(log_dir, \"reconstructed_audios.zip\")\n",
    "                with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "                    for file in audio_files:\n",
    "                        zipf.write(file, arcname=os.path.basename(file))\n",
    "                print(f\"Zipped audio files: {zip_path}\")\n",
    "\n",
    "        # Save the final model\n",
    "        save_model(model, train_dataset, val_dataset, {\n",
    "            'epochs': num_epochs,\n",
    "            'notes': 'VAE training',\n",
    "            'loss': 'MSE + KL Divergence',\n",
    "            'device': str(device)\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        print(\"\u26d4 Training could not start: No valid training data\")\n",
    "\n",
    "def save_model(model, train_data=None, val_data=None, metadata={}):\n",
    "    try:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        model_name = f\"melody_vae_{timestamp}.pt\"\n",
    "        save_path = os.path.join(OUTPUT_DIR, model_name)\n",
    "\n",
    "        torch.save({\n",
    "            'state_dict': model.state_dict(),\n",
    "            'config': {\n",
    "                'input_shape': model.input_shape,\n",
    "                'latent_dim': model.latent_dim\n",
    "            },\n",
    "            'metadata': {\n",
    "                'train_samples': len(train_data) if train_data else 0,\n",
    "                'val_samples': len(val_data) if val_data else 0,\n",
    "                'saved_at': timestamp,\n",
    "                **metadata\n",
    "            }\n",
    "        }, save_path)\n",
    "\n",
    "        print(f\"\u2705 Model successfully saved: {save_path}\")\n",
    "        return save_path\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Save error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}